{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":152187,"sourceType":"datasetVersion","datasetId":71232}],"dockerImageVersionId":12783,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Original data source **\n\n\nhttps://archive.ics.uci.edu/dataset/76/nursery","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Analysis\r\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using `matplotlib`. Depending on the data, not all plots will be made)","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:11:55.491257Z","iopub.execute_input":"2025-10-09T08:11:55.491575Z","iopub.status.idle":"2025-10-09T08:11:55.497244Z","shell.execute_reply.started":"2025-10-09T08:11:55.491525Z","shell.execute_reply":"2025-10-09T08:11:55.495989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder \nfrom sklearn.cluster import KMeans \nfrom sklearn.metrics import silhouette_score ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:11:57.37047Z","iopub.execute_input":"2025-10-09T08:11:57.370849Z","iopub.status.idle":"2025-10-09T08:11:57.376033Z","shell.execute_reply.started":"2025-10-09T08:11:57.370776Z","shell.execute_reply":"2025-10-09T08:11:57.374521Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There is 1 csv file in the current version of the dataset:\n","metadata":{}},{"cell_type":"code","source":"print(os.listdir('../input'))","metadata":{"_kg_hide-input":false,"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:11:59.365995Z","iopub.execute_input":"2025-10-09T08:11:59.36632Z","iopub.status.idle":"2025-10-09T08:11:59.371729Z","shell.execute_reply.started":"2025-10-09T08:11:59.366268Z","shell.execute_reply":"2025-10-09T08:11:59.370575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/nursery_data.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:12:01.674199Z","iopub.execute_input":"2025-10-09T08:12:01.674557Z","iopub.status.idle":"2025-10-09T08:12:01.7042Z","shell.execute_reply.started":"2025-10-09T08:12:01.67449Z","shell.execute_reply":"2025-10-09T08:12:01.702956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Original dataframe doesn't include title for each column, what makes it <br> <br> impossible to  intepret, therefore I add here column names\n\n\ndf.columns = [\"parents\", \"has_nurs\", \"form\", \"children\", \"housing\", \"finance\", \"social\", \"health\", \"class\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:12:05.869962Z","iopub.execute_input":"2025-10-09T08:12:05.870295Z","iopub.status.idle":"2025-10-09T08:12:05.876241Z","shell.execute_reply.started":"2025-10-09T08:12:05.870245Z","shell.execute_reply":"2025-10-09T08:12:05.875159Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###class column values\nclass value\tMeaning / closest interpretation\n\n1. not_recom\tChild is rejected — not recommended for admission\n2. recommend Child is accepted — regular acceptance\n3. very_recom Child is highly recommended / top priority — very strong case for admission\n4. priority. Child is given priority — admission is prioritized but not topmost\n5. spec_prior.Child has special priority — may include special needs, exceptional cases, very high prior\n","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T09:18:52.4738Z","iopub.execute_input":"2025-10-06T09:18:52.474091Z","iopub.status.idle":"2025-10-06T09:18:52.479324Z","shell.execute_reply.started":"2025-10-06T09:18:52.474045Z","shell.execute_reply":"2025-10-06T09:18:52.478482Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"12959 rows and 9 columns. We can process the whole dataset","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:41:34.836191Z","iopub.execute_input":"2025-10-06T08:41:34.83649Z","iopub.status.idle":"2025-10-06T08:41:34.880244Z","shell.execute_reply.started":"2025-10-06T08:41:34.836444Z","shell.execute_reply":"2025-10-06T08:41:34.879413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We have different types of columns as we can see, we will need to prepare them differently\ndf.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T09:20:09.420608Z","iopub.execute_input":"2025-10-06T09:20:09.420938Z","iopub.status.idle":"2025-10-06T09:20:09.427773Z","shell.execute_reply.started":"2025-10-06T09:20:09.420875Z","shell.execute_reply":"2025-10-06T09:20:09.426702Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"All columns' dtype are object, but from above line, I noticed column \"children\" seems to show number of childre. Let's investiage more to see if we need to convert dtype","metadata":{}},{"cell_type":"code","source":"#from the above dtype check, the number of children 'children' column show\n#dtype as <br> \n#object, let's see the unique value of this\nprint(df['children'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T09:19:57.613182Z","iopub.execute_input":"2025-10-06T09:19:57.61351Z","iopub.status.idle":"2025-10-06T09:19:57.618825Z","shell.execute_reply.started":"2025-10-06T09:19:57.613449Z","shell.execute_reply":"2025-10-06T09:19:57.617849Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Values for the \"children\" column are 1,2,3 and More. So not numerical value","metadata":{}},{"cell_type":"code","source":"for col in df.columns:\n    print(f\"Column: {col}\")\n    print(df[col].unique())\n    print(\"-\" * 40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:12:32.83119Z","iopub.execute_input":"2025-10-09T08:12:32.831569Z","iopub.status.idle":"2025-10-09T08:12:32.84857Z","shell.execute_reply.started":"2025-10-09T08:12:32.831505Z","shell.execute_reply":"2025-10-09T08:12:32.847151Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dataset is fictious dataset, target is to classification and selection of children to be accepted to daycare. Provided that number of daycare places are limited and not all children are accepted. The target column here is there for \"class\"","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T08:34:31.099055Z","iopub.status.idle":"2025-10-06T08:34:31.099529Z","shell.execute_reply":"2025-10-06T08:34:31.099254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"perfect, data is clean\n","metadata":{}},{"cell_type":"markdown","source":"**my idea is, from this dataset, obviously we can see the landscape of families for children born in the year. family size, economic situation, health condition. We can plot the data just to investigate to social economic pictures of the region**\n\n**origignally, dataset was made for ranking purpose and admision while the city had to many application to daycare. In today reality, we dont need to try hand pick children to see who is qualified for education. But the classification and other analysis can be made to understand the social factor and how to educate them at daycare. see what do they need more, how can school join hand with parents to help children develop fully and multifacetedly**\n","metadata":{}},{"cell_type":"code","source":"for col in df.columns:\n    plt.figure(figsize=(6,4))\n    \n    if df[col].dtype == 'object' or str(df[col].dtype) == 'category':\n        # Create bar plot and capture the Axes object\n        ax = df[col].value_counts().plot(kind='bar')\n        \n        plt.title(f\"Value Counts of {col}\")\n        plt.xlabel(col)\n        plt.ylabel(\"Count\")\n\n        # Add value labels on top of bars\n        for patch in ax.patches:\n            count = int(patch.get_height())\n            plt.text(patch.get_x() + patch.get_width()/2, count, count,\n                     ha='center', va='bottom')\n\n        plt.show()\n\n    else:\n        df[col].hist(bins=10)\n        plt.title(f\"Histogram of {col}\")\n        plt.xlabel(col)\n        plt.ylabel(\"Frequency\")\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T09:27:58.362847Z","iopub.execute_input":"2025-10-06T09:27:58.363188Z","iopub.status.idle":"2025-10-06T09:27:59.685125Z","shell.execute_reply.started":"2025-10-06T09:27:58.363126Z","shell.execute_reply":"2025-10-06T09:27:59.684196Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Above columns show equal portions for each category, which doesn't tell much about the demographic as I intended. Let's try an unsupervised K cluster analysis for discovery grouping taken all feature into consideration\n\nFirst we should encode the categorical data and delete the target column \"Class\"","metadata":{}},{"cell_type":"code","source":"X = pd.get_dummies(df)\nX.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:13:29.157428Z","iopub.execute_input":"2025-10-09T08:13:29.157755Z","iopub.status.idle":"2025-10-09T08:13:29.236588Z","shell.execute_reply.started":"2025-10-09T08:13:29.157706Z","shell.execute_reply":"2025-10-09T08:13:29.235509Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X=pd.get_dummies(df.drop('class', axis=1))\nX_cluster= X.copy()\nX_logit= X_cluster.assign(class_=df['class'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:25:11.531967Z","iopub.execute_input":"2025-10-09T08:25:11.532312Z","iopub.status.idle":"2025-10-09T08:25:11.563441Z","shell.execute_reply.started":"2025-10-09T08:25:11.532259Z","shell.execute_reply":"2025-10-09T08:25:11.5624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_logit.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:21:25.052887Z","iopub.execute_input":"2025-10-09T08:21:25.053204Z","iopub.status.idle":"2025-10-09T08:21:25.111673Z","shell.execute_reply.started":"2025-10-09T08:21:25.053155Z","shell.execute_reply":"2025-10-09T08:21:25.110058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now that we know how to build one model we can select the optimal k value\n# for this, we can iterate over k values, record the quality of the model for each k\nsse_clust = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(X_cluster)\n    sse_clust.append(kmeans.inertia_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T07:59:13.558251Z","iopub.execute_input":"2025-10-09T07:59:13.558614Z","iopub.status.idle":"2025-10-09T07:59:22.115975Z","shell.execute_reply.started":"2025-10-09T07:59:13.558546Z","shell.execute_reply":"2025-10-09T07:59:22.114546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(range(1, 11), sse_clust)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('SSE')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T07:59:34.232448Z","iopub.execute_input":"2025-10-09T07:59:34.232819Z","iopub.status.idle":"2025-10-09T07:59:34.391995Z","shell.execute_reply.started":"2025-10-09T07:59:34.232752Z","shell.execute_reply":"2025-10-09T07:59:34.390606Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I decided to use 6 Kclusters","metadata":{}},{"cell_type":"code","source":"# Fit K-Means\nkmeans = KMeans(n_clusters=6, random_state=42)\nlabels = kmeans.fit_predict(X_cluster)\n\n# Evaluate clustering\nscore = silhouette_score(X_cluster, labels)\nprint(\"Silhouette Score:\", score)\n\n# Optional: Compare clusters with real classes\ncomparison = pd.crosstab(df['class'], labels)\nprint(comparison)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T07:59:39.448917Z","iopub.execute_input":"2025-10-09T07:59:39.449271Z","iopub.status.idle":"2025-10-09T07:59:49.915118Z","shell.execute_reply.started":"2025-10-09T07:59:39.449221Z","shell.execute_reply":"2025-10-09T07:59:49.913873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_cluster)\n\nplt.figure(figsize=(8,6))\nplt.scatter(X_pca[:,0], X_pca[:,1], c=labels, cmap='tab10', s=10)\nplt.title(\"K-Means Clusters (PCA projection)\")\nplt.xlabel(\"PCA 1\")\nplt.ylabel(\"PCA 2\")\nplt.colorbar(label=\"Cluster\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T07:59:54.835409Z","iopub.execute_input":"2025-10-09T07:59:54.835761Z","iopub.status.idle":"2025-10-09T07:59:55.690295Z","shell.execute_reply.started":"2025-10-09T07:59:54.8357Z","shell.execute_reply":"2025-10-09T07:59:55.689277Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"together with low silhouette score and unclear grouping. The K cluster is not suitable for this data set","metadata":{}},{"cell_type":"code","source":"#try classification by regression and decision tree \n#first, let's import libraries\n# Classification performance evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\n\n# Logistic regression\nfrom sklearn.linear_model import LogisticRegression\n\n# Decision trees\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nimport graphviz\nfrom sklearn.ensemble import BaggingClassifier\n\n# Random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Grid search\nfrom sklearn.model_selection import GridSearchCV","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:20:26.315197Z","iopub.execute_input":"2025-10-09T08:20:26.315529Z","iopub.status.idle":"2025-10-09T08:20:26.322094Z","shell.execute_reply.started":"2025-10-09T08:20:26.315481Z","shell.execute_reply":"2025-10-09T08:20:26.320995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#test set and train set split\ncol_list = list(X_logit.columns)\ncol_list.remove('class_')\nX = X_logit[col_list]\ny = X_logit['class_']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:28:30.858152Z","iopub.execute_input":"2025-10-09T08:28:30.858487Z","iopub.status.idle":"2025-10-09T08:28:30.871804Z","shell.execute_reply.started":"2025-10-09T08:28:30.858439Z","shell.execute_reply":"2025-10-09T08:28:30.870771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:28:36.356196Z","iopub.execute_input":"2025-10-09T08:28:36.356505Z","iopub.status.idle":"2025-10-09T08:28:36.411075Z","shell.execute_reply.started":"2025-10-09T08:28:36.356458Z","shell.execute_reply":"2025-10-09T08:28:36.409973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train and evaluate logistic rgeression model\n# We have to increase iterations\n\ndata_logistic = LogisticRegression(max_iter = 500)\ndata_logistic.fit(X_train,y_train)\n\npred_logistic = data_logistic.predict(X_test)\nprint(confusion_matrix(y_test,pred_logistic))\n\n# The results are not very good, similar problem we faced before, one class is much smaller\n\nprint(classification_report(y_test,pred_logistic))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:28:42.908129Z","iopub.execute_input":"2025-10-09T08:28:42.908432Z","iopub.status.idle":"2025-10-09T08:28:43.174543Z","shell.execute_reply.started":"2025-10-09T08:28:42.908384Z","shell.execute_reply":"2025-10-09T08:28:43.171106Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"the precision, recall and therefore  f1_score are high for Not_recommendation, prority and spec_priority but the whole group of very_recom was mid-labeled to priority. The result of this is the child who should get the nursing place right away will be put to waiting list.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Compute confusion matrix\ncm = confusion_matrix(y_test, pred_logistic)\n\n# Define class names (replace with your actual class labels)\nclass_names = ['not_recom', 'priority', 'spec_prior', 'very_recom']\n\n# Create heatmap\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=class_names,\n            yticklabels=class_names)\n\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.show()\n\n# Print classification report\nprint(classification_report(y_test, pred_logistic, target_names=class_names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:34:24.097501Z","iopub.execute_input":"2025-10-09T08:34:24.097931Z","iopub.status.idle":"2025-10-09T08:34:24.634085Z","shell.execute_reply.started":"2025-10-09T08:34:24.097856Z","shell.execute_reply":"2025-10-09T08:34:24.632885Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\r!","metadata":{}}]}